# ğŸŒ€ CycleGAN: Unpaired Image-to-Image Translation

This project implements a **CycleGAN (Cycle-Consistent Adversarial Network)** for unpaired image-to-image translation, based on the paper  
[*Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks*](https://arxiv.org/abs/1703.10593) by Zhu et al., 2017.

CycleGAN enables transformation between two visual domains **without requiring paired training data** â€” for example, translating **horses â†” zebras**.

---

## ğŸ¯ Project Goals
1. Implement and understand the **CycleGAN architecture**.
2. Build the **Generator** and **Discriminator** networks from scratch using PyTorch.
3. Implement key **loss functions**:
   - Adversarial Loss  
   - Identity Loss  
   - Cycle Consistency Loss
4. Train a model that can translate **Horse â†” Zebra** images.

---

## ğŸ“˜ Learning Objectives
- Understand the concept of **unpaired image translation**.  
- Learn about **Cycle Consistency Loss** and **Identity Loss** in GANs.  
- Explore the structure of **Residual Blocks** in generators.  
- Implement a **PatchGAN discriminator**.  

---

## âš™ï¸ Getting Started

### 1. Requirements
Make sure you have the following Python libraries installed:

```bash
pip install torch torchvision matplotlib pillow tqdm
```
horse2zebra/
 â”œâ”€â”€ trainA/   # Horses
 â”œâ”€â”€ trainB/   # Zebras
 â”œâ”€â”€ testA/
 â””â”€â”€ testB/
